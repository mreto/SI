Regresja - metoda statystyczna pozwalajaca na badanie zwiazku pomiedzy wielkosciami danych. Umozliwia przewidywanie nieznanych wartosci jednych wielkosci na podstawie znanych wartosci innych. Formalnie regresja to dowolna metoda statystyczna pozwalajaca estymowac warunkowa wartosc oczekiwana zmiennej losowej, zwanej zmienna objasniana, dla zadanych wartosci innej zmiennej lub wektora zmiennych losowych (tzw. zmiennych objasniajacych). Uzycie regresji w praktyce sprowadza sie do dwoch faz: konstruowanie modelu - budowa tzw. modelu regresyjnego, czyli funkcji opisujacej, jak zalezy wartosc oczekiwana zmiennej objasnianej od zmiennych objasniajacych. Funkcja ta moze byc zadana nie tylko czystym wzorem matematycznym, ale takze calym algorytmem, np. w postaci drzewa regresyjnego, sieci neuronowej, itp.. Model konstruuje sie tak, aby jak najlepiej pasowal do danych z proby, zawierajacej zarowno zmienne objasniajace, jak i objasniane (tzw. zbior uczacy). Mowiac o wyliczaniu regresji ma sie na mysli te faze. stosowanie modelu (tzw. scoring) - uzycie wyliczonego modelu do danych w ktorych znamy tylko zmienne objasniajace, w celu wyznaczenia wartosci oczekiwanej zmiennej objasnianej. Dzial statystyki zajmujacy sie modelami i metodami regresji zwany jest analiza regresji. Regresja, w ktorej wystepuje wiecej niz jedna zmienna objasniajaca, zwana jest regresja wieloraka (ang. multiple regression).   == Globalne modele parametryczne == W modelach parametrycznych ogolna postac modelu jest zalozona z gory, a celem procedury regresji jest dobranie takich jej parametrow, ktore definiowalyby funkcje mozliwie dobrze odpowiadajaca probie uczacej. Zwykle stosuje sie tzw. globalne modele parametryczne, gdzie wartosci wspolczynnikow sa takie same dla dowolnych wartosci zmiennych objasniajacych.   === Ogolna postac modelu === W zapisie formalnym model przybiera zwykle postac:                         Y         =         f         (         X         ,         b         )         +         e                 {\displaystyle Y=f(X,\beta )+\varepsilon }    gdzie:                         X                 {\displaystyle X}    - wektor zmiennych objasniajacych (predyktorow),                         Y                 {\displaystyle Y}    - zmienna objasniana,                         b                 {\displaystyle \beta }    - wektor wspolczynnikow regresji (zwykle bedacych liczbami rzeczywistymi)                         f         (         X         ,         b         )                 {\displaystyle f(X,\beta )}    - funkcja regresji o wartosciach w liczbach rzeczywistych,                         e                 {\displaystyle \varepsilon }    - blad losowy, o rozkladzie byc moze zaleznym od                         X                 {\displaystyle X}   , przy czym                                    E                  (         e                    |                  X         )         =         0                 {\displaystyle \mathbb {E} (\varepsilon |X)=0}    oraz                                    sup                        X                             Var         [?]         (                    e                        X                                        |                  X         )         <         [?]                 {\displaystyle \sup _{X}\operatorname {Var} (\varepsilon _{X}|X)<\infty }   . Dzieki temu                                    E                  Y         =         f         (         X         ,         b         )                 {\displaystyle \mathbb {E} Y=f(X,\beta )}     Niekiedy wprowadza sie do modelu takze blad zmiennych objasniajacych. Wzor zwykle przybiera wowczas forme:                         Y         =         f         (         X         +                    e                        X                             ,         b         )         +         e                           {\displaystyle Y=f(X+\varepsilon _{X},\beta )+\varepsilon \ }      === Miara bledu === Celem konstrukcji modelu jest przyblizenie nieznanej funkcji                         f                           {\displaystyle f\ }    przez jej estymator                                                                f               ^                                                  {\displaystyle {\widehat {f}}}   . Sprowadza sie to do takiego wyznaczenia wektora wspolczynnikow                         b                 {\displaystyle \beta }   , aby zminimalizowac w zbiorze uczacym funkcje straty                         L         (                                                f               ^                                          ,         f         )         =                                 1             n                                        [?]                        i             =             1                                   n                             D         (                                                f               ^                                          (                    x                        i                             )         ,         f         (                    x                        i                             )         )                 {\displaystyle L({\widehat {f}},f)={\frac {1}{n}}\sum _{i=1}^{n}\Delta ({\widehat {f}}(x_{i}),f(x_{i}))}    gdzie                         D         (         a         ,         b         )                           {\displaystyle \Delta (a,b)\ }    jest ustalona miara odleglosci miedzy wartosciami                         a                 {\displaystyle a}    i                         b                 {\displaystyle b}    (tzw. miara bledu). Wybor miary                         D         (         a         ,         b         )                           {\displaystyle \Delta (a,b)\ }    bardzo wplywa na algorytm i wyniki regresji. Zwykle jako miare bledow stosuje sie sume kwadratow roznic (bledow regresji):                         D         (         a         ,         b         )         =         (         a         -         b                    )                        2                                               {\displaystyle \Delta (a,b)=(a-b)^{2}\ }    gdyz wowczas obliczenia sa najprostsze - dopasowanie modelu sprowadza sie do zastosowania prostej matematycznie metody najmniejszych kwadratow. Ma to jednak swoja wade - kwadrat bledow duzo silniej zalezy od obserwacji dla ktorych blad jest najwiekszy niz od tych, do ktorych model dobrze sie dopasowal. Metoda najmniejszych kwadratow daje wiec niedokladne lub wrecz zafalszowane wyniki, jesli w zbiorze uczacym wystepuja obserwacje zbyt dalekie od sredniej, tzw. elementy odstajace (np. pomylki przy wprowadzaniu danych). W zwiazku z tym stosowane sa takze inne miary bledow, bardziej odporne, takie jak np. wartosc bezwzgledna roznicy.   === Najpopularniejsze modele parametryczne ===   ==== Regresja liniowa ====  Model regresji liniowej ma postac                         Y         =                    b                        0                             +                    x                        1                                        b                        1                             +                    x                        2                                        b                        2                             +         [?]         +                    x                        n                                        b                        n                             +         e                 {\displaystyle Y=\beta _{0}+x_{1}\beta _{1}+x_{2}\beta _{2}+\dots +x_{n}\beta _{n}+\varepsilon }    Wowczas algorytmem obliczania wspolczynnikow modelu jest metoda najmniejszych kwadratow (w przypadku wariancji jako miary bledu) albo np. metoda najwiekszej wiarygodnosci dla innych miar.   ==== Regresja nieliniowa ==== Regresja, w ktorej postac modelu dopuszcza nieliniowa zaleznosc pomiedzy zmiennymi objasniajacymi a zmienna objasniana. Stosowane sa rozne modele, budowane na potrzeby konkretnego przypadku. Dla jednej zmiennej objasniajacej                         Z                 {\displaystyle Z}    moze to byc na przyklad:                         Y         =                    b                        0                             +         Z                    b                        1                             +                    Z                        2                                        b                        2                             +                    Z                        3                                        b                        3                             +         e                 {\displaystyle Y=\beta _{0}+Z\beta _{1}+Z^{2}\beta _{2}+Z^{3}\beta _{3}+\varepsilon }    Jak latwo zauwazyc model ten daje sie sprowadzic do regresji liniowej przez utworzenie sztucznych zmiennych objasniajacych                                    X                        1                             =         Z                 {\displaystyle X_{1}=Z}   ,                                    X                        2                             =                    Z                        2                                     {\displaystyle X_{2}=Z^{2}}   ,                                    X                        3                             =                    Z                        3                                     {\displaystyle X_{3}=Z^{3}}   . Regresja liniowa dopasuje wowczas do danych wielomian trzeciego stopnia zamiast prostej. Mozna stosowac takze inne funkcje sprowadzajace model do postaci liniowej, np. logarytm.   ==== Modele z interakcjami ==== Model regresji liniowej mozna rowniez rozszerzyc w inny sposob, wprowadzajac do niego jako sztucznie stworzone predyktory np. iloczyny dwoch lub wiekszej liczby zmiennych objasniajacych. Pozwala to na uwzglednienie tzw. interakcji pomiedzy zmiennymi, czyli zmiany sily wplywu jednej ze zmiennych przy roznych wartosciach innej zmiennej.   ==== Uogolnione modele liniowe (GLM) ==== W modelach tych przyjmuje sie nastepujace zalozenia: Zmienne objasniajace wplywaja na zmienna objasniana tylko przez tzw. skladnik systematyczny                         e         =                    X                        T                             b                 {\displaystyle \eta =X^{T}\beta }     gdzie                                    X                        T                                     {\displaystyle X^{T}}    oznacza transpozycje wektora                         X                 {\displaystyle X}    Rozklad prawdopodobienstwa zmiennej objasnianej jest okreslony przez tzw. skladnik losowy modelu:                         Y         ~         N         (         m         ,                    s                        2                             )         ,                    E                  Y         =         m                 {\displaystyle Y\sim N(\mu ,\sigma ^{2}),\mathbb {E} Y=\mu }     Wartosc oczekiwana                         m                 {\displaystyle \mu }    skladnika losowego zalezy od skladnika systematycznego w sposob okreslony przez tzw. funkcje wiazaca                         l                 {\displaystyle l}   :                         e         =         l         (         m         )                 {\displaystyle \eta =l(\mu )}     W zaleznosci od wyboru funkcji wiazacej otrzymuje sie rozne modele. Nieznane parametry                         b                 {\displaystyle \beta }    sa zwykle estymowane za pomoca metod najwiekszej wiarygodnosci, quasi-najwiekszej wiarygodnosci, lub metod bayesowskich.   ==== Regresja logistyczna ====  Szczegolny przypadek GLM, stosowany, gdy zmienna objasniana                         Y                 {\displaystyle Y}    przyjmuje tylko dwie wartosci (zwykle oznaczane 0 i 1), np. mowi, czy prognozowane zdarzenie bedzie mialo miejsce. Funkcja wiazaca jest w tym przypadku logit.   == Regresja nieparametryczna == Alternatywna koncepcja jest regresja nieparametryczna. Metody regresji nieparametrycznej nie zakladaja, ze estymowana funkcja                         f                 {\displaystyle f}    jest znana z dokladnoscia do skonczenie wielu estymowalnych parametrow. Tym samym sa czesto bardziej elastyczne w poszukiwaniu rozwiazan. Z drugiej strony w regresji parametrycznej o wiele prostszy jest matematyczny opis modelu, co pozwala na przyklad na latwe wyznaczanie przedzialow ufnosci prognozowanej wartosci. W regresji nieparametrycznej bywa to trudniejsze.   == Krokowa konstrukcja modelu regresji == Metody regresji krokowej (ang. stepwise regression) sa sposobem na wybranie zmiennych objasniajacych do modelu.   === Regresja krokowa postepujaca === W tej wersji zmienne sa kolejno dodawane do modelu. Przykladowo moze ona polegac w pierwszym kroku na wyborze do modelu tej zmiennej objasniajacej, ktora jest najsilniej skorelowana ze zmienna objasniana i wyznacza model o istotnych parametrach. W drugim kroku wybierana jest kolejna zmienna objasniajaca, ktorej wartosci sa najsilniej skorelowane z resztami kroku pierwszego, a rozszerzony model charakteryzuje sie istotnoscia wszystkich parametrow. Oprocz istotnosci parametrow bada sie rowniez istotnosc wspolczynnika determinacji. Procedura podlega zakonczeniu, gdy zabraknie zmiennych objasniajacych lub dolaczenie nowej zmiennej do rownania prowadzi do utraty waloru istotnosci przez parametry lub wspolczynnik determinacji.   === Regresja krokowa wsteczna === Polega w pierwszym kroku na skonstruowaniu modelu zawierajacego wszystkie potencjalne zmienne objasniajace, a nastepnie na stopniowym eliminowaniu zmiennych tak, aby utrzymac model z najwyzsza wartoscia wspolczynnika determinacji przy zachowaniu istotnosci parametrow. Istnieja tez metody mieszane, w ktorych algorytm zarowno dodaje, jak i usuwa zmienne w kolejnych krokach.   == Bibliografia == Jacek Koronacki, Jan Cwik: Statystyczne systemy uczace sie. Warszawa: Wydawnictwa Naukowo-Techniczne, 2005. ISBN 83-204-3157-3.   == Przypisy ==   == Zobacz tez == regresja liniowa wspolczynnik regresji regresja pozorna autoregresja